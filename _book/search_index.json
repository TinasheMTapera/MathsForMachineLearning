[
["index.html", "Mathematics for Machine Learning in R Chapter 1 Pre-amble", " Mathematics for Machine Learning in R Tinashe M. Tapera 2020-07-18 Chapter 1 Pre-amble My name is Tinashe. I’m applying to PhD Programs soon. I’m not quite sure when, but soon. I’m interested in applying machine learning methods to understand human-based or human-generated data — like the relationship between behaviour and the brain. And for my PhD fields of interest (biostatistics, computational neuroscience, machine learning, quantitative psychology — in no particular order), I’ve been made aware that a solid foundation in linear algebra and calculus is an agreed upon prerequisite for success in applying, and success in the field at large. Fair enough, I used to say. I know most of calc and algebra. I was part of the International Baccalaureate Diploma Programme in high school, and we did a lot of maths (and yes, I pronounce it maths). As it turns out, not only is “I did it before” not sufficient documentation of one’s mathematical acumen, but it’s recommended to have it on one’s transcript from the degree level. And by “recommended”, they really mean “necessary”. Which is a fair prerequisite — I get it. However, being an international student in the US with a lot of logistical barriers to think about, I’ve found myself in the unique position of wanting to apply to PhD programs in quantitative fields, and having nothing to prove my grasp of the mathematics of calculus and linear algebra. This could be an insurmountable challenge, but not to worry! One advantage of being born in this day-and-age is that everything is available on the internet somewhere — and that includes learning resources for college calculus and linear algebra! With the immense amount of learning material available online, I’m certainly able to independantly re-learn and self-teach many of the concepts from traditional college courses, and document my learning and progress online as well! With that being said, allow me to outline the purpose of this project: To learn/re-learn the foundational concepts of calculus and linear algebra necessary for machine learning applications at graduate level study Overall, the IBDP is an excellent high-school track that preps its participants for greater success in college courses. Having done IB, I got through college pre-med physics without a hitch. However, there’s no guarantee that the knowledge from the IBDP retains through the 4 (in my case 5) years of college post. Additionally, the material covered in IB maths is not absolutely inclusive of the material in a college level linear algebra or calculus sequence. It’s close, but doesn’t quite cover it all. And so, it would definitely be to my benefit to learn and document my appreciation of these topics. To document my learning of these topics Perhaps the most important reason for this project is to be able to have living, accessible documentation of the learning process. The R community has been growing steadily in its applicability of the language to all sorts of different interesting problems — one of them being the publication of reproducible narrative coding with Rmarkdown. With this package, R programmers can write reproducible code notebooks that can be rendered into reports with a myriad of fun features. In this project, I aim to use Rmarkdown to document my learning, including notes, homework exercises, and exams, with personalised examples of the thought process involved in solving problems, just as I used to do in secondary school (where we had one “book” for a subject; you take notes in the front, write homework/exams/assignments in the back, and hand it in every Friday so the teacher can check both, and mark your work). To practice publication with R and Rmarkdown It doesn’t hurt to have a another repository with a cool R project — if I’m going to be re-learning calculus and linear algebra, things I already don’t really like, I figure why not make it interesting by trying to do it in R? Although it started purely as a statistical language, the R community is growing and reaching its fingers into all kinds of niche uses, like machine learning, deep learning, analytical dashboards, geolocation, scientific publication, Natural Language Processing, economics, database programming, slide presentations, 3D data vis, production software development — heck, if you want, you can use R to send you, or even create, memes on the fly. It’s a fun time to be an R programmer, and contributing to the growing community in my own way is something I’m very motivated to do. For others to see it may be possible for them too In recompense to how the R community has benefitted my journey, I can only hope that this documentation may serve to inspire other R learners, stats learners, or anybody who’d like to tackle calculus and linear algebra, to believe that self-learning is an effective way to understand and appreciate the material. The world is changing, and perhaps it’s time we paid more attention to prospective academics with broader and more diverse academic journeys. So with that, I introduce you to Mathematics for Machine Learning in R. This repository follows Mathematics for Machine Learning by Deisenroth, Faisal, and Ong, available for free here. The basic premise will follow that mentioned before: For each chapter in the book, I will “take notes” by summarising and commenting on the concept being taught. At the end of each chapter, I’ll answer the available questions in the book, and give code examples where appropriate. library(tidyverse) "],
["linear-algebra.html", "Chapter 2 Linear Algebra 2.1 Introduction 2.2 Systems of Linear Equations 2.3 Matrices 2.4 Solving Systems of Linear Equations", " Chapter 2 Linear Algebra 2.1 Introduction Let’s first talk about the foundation of most of the maths that happens in machine learning: vectors. For most of us, vectors show up first in gemoetry and physics in school — in practice, they’re used to surmise how objects move on a plane (think, the problem of calculating what time a ship will arrive somewhere if it travels in a striaght line against wind resistance). These are known as geometric vectors, but there are also other kinds of vectors. In general, a vector is defined as objects that 1) can be added together, and 2) can be multiplied by a scalar number 1. That’s pretty much it! Given that definition, the following are all vectors: Geometric vectors, like the line on a cartesian plane with a direction and magnitude Polynomial numbers (they can be added and multiplied) Audio signals – they can be added and multiplied too! Tuples in programming! my_vector &lt;- c(1, 5, 7) print(my_vector * 100) ## [1] 100 500 700 my_vector2 &lt;- c(8, 3, -1) print(my_vector + my_vector2) ## [1] 9 8 6 Just remember, to add two vectors, they must be the same length, and each element of the vector is added component-wise; likewise, multiplying a vector by a scalar means that each element is multiplied component-wise. And importantly, when it comes to machine learning, it’s important that all the possible elements that can result from the addition and multiplication of vectors and scalars, are themselves part of a countably finite set of numbers — that is to say, there should be an array of numbers that describe every possible outcome (even if you can’t think of it at the moment). This is called a “vector space” 2. Now that we’ve discussed vectors, we’ll discuss linear equations. 2.2 Systems of Linear Equations Solving systems of linear equations is one of those mathematical skills that seems to apply to a very narrow and specific set of problems. You do it for a year or two when you’re 14, and after that you kinda just, move on. But in truth, the only reason we do this is because the real-world application is vastly too large for any human being to spend any amount of time solving – we leave it up to the computers. But it’s important that we understand what’s going on, because that’s why we’re here! So let’s review: Recall that a linear equation is one that takes the form \\(y = mx + c\\) (or however you learned to specify it in school), that can be used to describe a line on a graph. If you have enough information, you can solve for unknown variables – yay algebra. A system of linear equations is the case wherein you have multiple expressions with the same unknown variables, and you use them all to solve for them. This is often called the geometric interpretation of systems of linear equations, and on a graph with two planes/variables, the solution for the system is the intersection of the two lines. For example: \\[\\begin{align*} x_1 + x_2 + x_3 = 3 &amp;&amp;\\text{(1)}\\\\ x_1 - x_2 + 2x_3 = 2 &amp;&amp;\\text{(2)}\\\\ x_2 + x_3 = 2 &amp;&amp;\\text{(3)} \\end{align*}\\] Remember how to do these? To solve for all three variables, we have to combine the equations together and eliminate variables, so that we can isolate only one variable to solve. If you combine (1) and (3), it’s clear that \\(x_1\\) has to be 1. If you combine (1) and (2), the \\(x_2\\) variable cancels out, and you’re left with \\(2x_1 + 3x_3 = 5\\), and if \\(x_1\\) is 1, then \\(2 + 3x_3 = 5\\), and \\(x_3\\) is therefore also 1. Lastly, we plug the values for \\(x_1\\) and \\(x_3\\) into any equation, and get that \\(x_2\\) is also 1. So the unique solution for this system of linear equations, is (1,1,1). In a 3 dimensional graph, you’d see this: In another example: \\[\\begin{align*} x_1 + x_2 + x_3 =3 &amp;&amp;\\text{(1)}\\\\ x_1 - x_2 + 2x_3 =2 &amp;&amp;\\text{(2)}\\\\ 2x_1 + 3x_3 = 1 &amp;&amp;\\text{(3)} \\end{align*}\\] If you try and apply the typical approach to this system of linear equations, you’ll find that there is no solution – the planes on a graph wouldn’t intersect. Linear regression solves this version of systems of linear equations especially well. When we generalise these systems, we can use the following notations: \\[\\begin{equation*} x_1 \\begin{bmatrix} a_{1,1} \\\\ \\vdots \\\\ a_{m,1} \\end{bmatrix} + x_2 \\begin{bmatrix} a_{1,2} \\\\ \\vdots \\\\ a_{m,2} \\end{bmatrix} + \\cdots + x_n \\begin{bmatrix} a_{1,n} \\\\ \\vdots \\\\ a_{m,n} \\end{bmatrix} = \\begin{bmatrix} b_{1} \\\\ \\vdots \\\\ b_{m} \\end{bmatrix} \\end{equation*}\\] Or, \\[\\begin{equation*} \\begin{bmatrix} a_{1,1} &amp; \\cdots &amp; a_{1,n}\\\\ \\vdots &amp; &amp; \\vdots \\\\ a_{1,m} &amp; \\cdots &amp; a_{m,n} \\end{bmatrix} \\begin{bmatrix} x_{1} \\\\ \\vdots \\\\ x_{n} \\end{bmatrix} = \\begin{bmatrix} b_{1} \\\\ \\vdots \\\\ b_{m} \\end{bmatrix} \\end{equation*}\\] And yes, those are matrices. 2.2.1 Solving Systems of Linear Equations in R It turns out solving equations is actually a pretty simple task in R, just not a common one. If we convert the first equation example to the matrix form above, it looks like this: # the coefficients a &lt;- matrix(c(1, 1, 1, 1, -1, 2, 0, 1, 1), byrow = T, nrow = 3) print(a) ## [,1] [,2] [,3] ## [1,] 1 1 1 ## [2,] 1 -1 2 ## [3,] 0 1 1 # the answers b &lt;- matrix(c(3, 2, 2), byrow = T, nrow = 3) print(b) ## [,1] ## [1,] 3 ## [2,] 2 ## [3,] 2 Now, with these two matrix objects, we can use the solve() function: # the unkowns unknowns &lt;- solve(a, b) print(unknowns) ## [,1] ## [1,] 1 ## [2,] 1 ## [3,] 1 Pretty simple! We’re going to revisit more difficult scenarios to solve, but first we’ll need to refresh our memories on matrices: 2.3 Matrices Matrices are great ways to represent systems of linear equations or linear functions. Formally, a matrix looks like this: \\[\\begin{equation*} A = \\begin{bmatrix} a_{1,1} &amp; a_{1,2} &amp; \\cdots &amp; a_{1,n} \\\\ a_{2,1} &amp; a_{2,2} &amp; \\cdots &amp; a_{2,n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{m,1} &amp; a_{m,2} &amp; \\cdots &amp; a_{m,n} \\end{bmatrix}, a_{mn} \\in \\mathbb{R} \\end{equation*}\\] Recall that, so long as the elements of the matrix are real numbers (\\(a_{mn} \\in \\mathbb{R}\\)), then addition of two matrices is element-wise. a &lt;- matrix(c(1,2,1,4,4,5,6,7,7), nrow = 3, ncol = 3) print(a) ## [,1] [,2] [,3] ## [1,] 1 4 6 ## [2,] 2 4 7 ## [3,] 1 5 7 b &lt;- matrix(c(-7,-7,6,2,1,-1,4,5,-4), nrow = 3, ncol = 3) print(b) ## [,1] [,2] [,3] ## [1,] -7 2 4 ## [2,] -7 1 5 ## [3,] 6 -1 -4 print(a + b) ## [,1] [,2] [,3] ## [1,] -6 6 10 ## [2,] -5 5 12 ## [3,] 7 4 3 Matrices can of course be multiplied by a scalar: print(100 * a) ## [,1] [,2] [,3] ## [1,] 100 400 600 ## [2,] 200 400 700 ## [3,] 100 500 700 Multiplication can only happen if the matrices share the same dimension for the first matrix’s columns, and the second matrix’s rows. That is to say, \\(AB = C\\) only if \\(A \\in \\mathbb{R}^{m \\times n}, B \\in \\mathbb{R}^{n \\times k}\\) – the number of columns of A (\\(n\\)) is the same as the number of rows of B (\\(n\\)). As I learnt it “the inner dimensions must be equal”. Interestingly, to know the final shape of the resulant matrix of multiplication, you use “outer dimentions”, meaning that \\(C\\) will be a matrix with dimensions \\((m,k)\\). This is called a dot product multiplication and is more common than the special case of matrix multiplication called the Hadamard product. There’s a specifically long process of calculating these by hand, that includes skimming row-wise and column-wise along each element and summing the values at the end — I won’t do that here but it’ll be pretty familiar for anybody who did it in high school [INSERT LINK TO DEMO]. # a 3 x 2 matrix a &lt;- matrix(c(1,2,1,4,4,5), nrow = 3, ncol = 2) print(a) ## [,1] [,2] ## [1,] 1 4 ## [2,] 2 4 ## [3,] 1 5 # a 2 x 3 matrix b &lt;- matrix(c(-7,-7,6,2,1,-1), nrow = 2, ncol = 3) print(b) ## [,1] [,2] [,3] ## [1,] -7 6 1 ## [2,] -7 2 -1 # they can multiply because the inner dimensions are the same, 2 x 2 # the result is a 3 x 3 matrix print(a %*% b) ## [,1] [,2] [,3] ## [1,] -35 14 -3 ## [2,] -42 20 -2 ## [3,] -42 16 -4 When you have a square matrix (dimensions \\((n, n)\\)), there’s a special case of matrix called the identity matrix that has 1 values only along it’s diagonal from left to right: \\[\\begin{equation*} I := \\begin{bmatrix} 1 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; 1 &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; 1 \\end{bmatrix}, \\in \\mathbb{R}^{n \\times n} \\end{equation*}\\] In R: diag(3) ## [,1] [,2] [,3] ## [1,] 1 0 0 ## [2,] 0 1 0 ## [3,] 0 0 1 It comes in handy for a couple of different applications. Here are some general rules about matrices: They are not commutative. Unlike in general algebra, you can’t reverse the order of multiplication: \\(A \\cdot B \\neq B \\cdot A\\). This is because of the inner dimensions/outer dimensions thing, as well as how the element sums are calculated. # a 3 x 2 matrix a &lt;- matrix(c(1,2,1,4,4,5), nrow = 3, ncol = 2) print(a) ## [,1] [,2] ## [1,] 1 4 ## [2,] 2 4 ## [3,] 1 5 # a 2 x 2 matrix b &lt;- matrix(c(-7,-7,6,2), nrow = 2, ncol = 2) print(b) ## [,1] [,2] ## [1,] -7 6 ## [2,] -7 2 # the result is a 3 x 2 matrix print(a %*% b) ## [,1] [,2] ## [1,] -35 14 ## [2,] -42 20 ## [3,] -42 16 # but flipping the order can&#39;t be done because now you&#39;re trying to multiply a 2x2 and 3x2 try(b %*% a) ## Error in b %*% a : non-conformable arguments They are associative. This means that if you have multiple matrices being multiplied, you can do the multiplications in any order (so long as the overall order of the matrices is kept the same): \\(A(BC) = (AB)C\\) # a 3 x 2 matrix a &lt;- matrix(c(1,2,1,4,4,5), nrow = 3, ncol = 2) # a 2 x 2 matrix b &lt;- matrix(c(-7,-7,6,2), nrow = 2, ncol = 2) print((3 * a) %*% b) ## [,1] [,2] ## [1,] -105 42 ## [2,] -126 60 ## [3,] -126 48 print(a %*% (3 * b)) ## [,1] [,2] ## [1,] -105 42 ## [2,] -126 60 ## [3,] -126 48 They are distributive. This means a common factor can be distributed over all of the elements of an operation: \\((A+B)C=AC+BC\\) or \\(A(C+D)=AC+AD\\) # a 2 x 2 matrix a &lt;- matrix(c(1,2,1,4), nrow = 2, ncol = 2) # a 2 x 2 matrix b &lt;- matrix(c(-7,-7,6,2), nrow = 2, ncol = 2) print((a + b) * 100) ## [,1] [,2] ## [1,] -600 700 ## [2,] -500 600 print(100 * (a + b)) ## [,1] [,2] ## [1,] -600 700 ## [2,] -500 600 print(100*a + 100*b) ## [,1] [,2] ## [1,] -600 700 ## [2,] -500 600 Multiplication by the Identity matrix is important. If you have a matrix \\(A \\in \\mathbb{R}^{m \\times n}\\), then you can multiply it by the identity matrix twice to get it back: \\(I_mA=AI_n =A\\); there are many reasons why the Identity matrix is important, the first being the solution to the inverse of a matrix. 2.3.1 Inverses and Transpositions Let’s say we have a square matrix, \\(A\\), and a similarly shaped square matrix \\(B\\); if we multiply \\(A\\) and \\(B\\), and the result is the identity matrix \\(I\\), then B is referred to as the inverse of \\(A\\), with the notation \\(A^{-1}\\). In this specific case, \\(AB = I_n = BA\\), where \\(n\\) denotes the dimensions of this square matrix: a &lt;- matrix(c(1,2,1,4,4,5,6,7,7), nrow = 3, ncol = 3) print(a) ## [,1] [,2] [,3] ## [1,] 1 4 6 ## [2,] 2 4 7 ## [3,] 1 5 7 b &lt;- matrix(c(-7,-7,6,2,1,-1,4,5,-4), nrow = 3, ncol = 3) print(b) ## [,1] [,2] [,3] ## [1,] -7 2 4 ## [2,] -7 1 5 ## [3,] 6 -1 -4 identity &lt;- diag(3) print(identity) ## [,1] [,2] [,3] ## [1,] 1 0 0 ## [2,] 0 1 0 ## [3,] 0 0 1 print(a %*% b) ## [,1] [,2] [,3] ## [1,] 1 0 0 ## [2,] 0 1 0 ## [3,] 0 0 1 print(a %*% b == identity) ## [,1] [,2] [,3] ## [1,] TRUE TRUE TRUE ## [2,] TRUE TRUE TRUE ## [3,] TRUE TRUE TRUE Importantly, not every matrix has an inverse, due to the nature of the mathematics used to calculate the inverse. I’m not going to write every step of this procedure, but just recall that in order to find the inverse of a matrix (for example, \\(A \\in \\mathbb{R}^{2 \\times 2}\\)) by hand, you end up with this as the final step: \\[\\begin{equation*} A^{-1} = \\frac{1}{a_{1,1}a_{2,2} - a_{1,2}a_{2,1}} \\begin{bmatrix} a_{1,1} &amp; a_{1,2} \\\\ a_{2,1} &amp; a_{2,2} \\end{bmatrix} \\end{equation*}\\] The problem here is that if the denominator \\(a_{1,1}a_{2,2} - a_{1,2}a_{2,1}\\) (formally known as the determinant) is zero, there is no solution. Such a matrix is called singular. a &lt;- matrix(c(1,2,1,4,4,5,6,7,7), nrow = 3, ncol = 3) det(a) %&gt;% # function to find the determinant of a matrix print() ## [1] 1 b &lt;- matrix(c(-7,-7,6,2), nrow = 2, ncol = 2) det(b) %&gt;% print() ## [1] 28 To find the transpose of a matrix, use t() in R; otherwise, by hand, the transposition of a matrix is, in a sense, a mirror image of the matrix, where each column becomes a row and each row becomes a column: a &lt;- matrix(c(1,2,1,4,4,5), nrow = 3, ncol = 2) print(a) ## [,1] [,2] ## [1,] 1 4 ## [2,] 2 4 ## [3,] 1 5 t(a) %&gt;% print() ## [,1] [,2] [,3] ## [1,] 1 2 1 ## [2,] 4 4 5 This is denoted formally with this symbol: \\(A^{\\intercal}\\). If you had a matrix where the transpose happens to be the exact same as the input, the matrix is said to be symmetric. # Here&#39;s a trick to create a symmetrical matrix, using the upper.tri() masking function a &lt;- matrix(1:25, 5) print(a) ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 6 11 16 21 ## [2,] 2 7 12 17 22 ## [3,] 3 8 13 18 23 ## [4,] 4 9 14 19 24 ## [5,] 5 10 15 20 25 # find the &quot;upper triangle&quot; of a matrix object: upper.tri(a) %&gt;% print() ## [,1] [,2] [,3] [,4] [,5] ## [1,] FALSE TRUE TRUE TRUE TRUE ## [2,] FALSE FALSE TRUE TRUE TRUE ## [3,] FALSE FALSE FALSE TRUE TRUE ## [4,] FALSE FALSE FALSE FALSE TRUE ## [5,] FALSE FALSE FALSE FALSE FALSE # we transpose the matrix and take the upper.tri() of the original, and paste it into the transposed spots t(a)[upper.tri(a)] %&gt;% print() ## [1] 2 3 8 4 9 14 5 10 15 20 # putting it all together: a[upper.tri(a)] &lt;- t(a)[upper.tri(a)] print(a) ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 2 3 4 5 ## [2,] 2 7 8 9 10 ## [3,] 3 8 13 14 15 ## [4,] 4 9 14 19 20 ## [5,] 5 10 15 20 25 # now, test if the inverse is the same print(a == t(a)) ## [,1] [,2] [,3] [,4] [,5] ## [1,] TRUE TRUE TRUE TRUE TRUE ## [2,] TRUE TRUE TRUE TRUE TRUE ## [3,] TRUE TRUE TRUE TRUE TRUE ## [4,] TRUE TRUE TRUE TRUE TRUE ## [5,] TRUE TRUE TRUE TRUE TRUE In sum, here are some laws for matrix inversion and transposition: \\(AA^{-1} = I = A^{-1}A\\) a &lt;- as.matrix(cbind(c(3,0),c(1,2))) print(a) ## [,1] [,2] ## [1,] 3 1 ## [2,] 0 2 print(a %*% solve(a)) ## [,1] [,2] ## [1,] 1 0 ## [2,] 0 1 \\((AB)^{-1} = B^{-1}A^{-1}\\) b &lt;- as.matrix(cbind(c(4,1),c(0,2))) print(b) ## [,1] [,2] ## [1,] 4 0 ## [2,] 1 2 print(a %*% b) ## [,1] [,2] ## [1,] 13 2 ## [2,] 2 4 print(solve(a %*% b) == (solve(b) %*% solve(a))) ## [,1] [,2] ## [1,] TRUE TRUE ## [2,] TRUE TRUE \\((A+B)^{-1} \\neq B^{-1}A^{-1}\\) print(a + b) ## [,1] [,2] ## [1,] 7 1 ## [2,] 1 4 print(a + b == (solve(b) %*% solve(a))) ## [,1] [,2] ## [1,] FALSE FALSE ## [2,] FALSE FALSE \\((A^\\intercal)^\\intercal = A\\) print(a) ## [,1] [,2] ## [1,] 3 1 ## [2,] 0 2 print(t(t(a))) ## [,1] [,2] ## [1,] 3 1 ## [2,] 0 2 \\((A+B)^\\intercal = A^{\\intercal}+B^{\\intercal}\\) print(t(a + b)) ## [,1] [,2] ## [1,] 7 1 ## [2,] 1 4 print(t(a) + t(b)) ## [,1] [,2] ## [1,] 7 1 ## [2,] 1 4 \\((AB)^{\\intercal} = B^{\\intercal}A^{\\intercal}\\) print(t(a %*% b)) ## [,1] [,2] ## [1,] 13 2 ## [2,] 2 4 print(t(b) %*% t(a)) ## [,1] [,2] ## [1,] 13 2 ## [2,] 2 4 2.4 Solving Systems of Linear Equations Now with our refreshed understanding of matrices, let’s revisit how to solve systems of linear equations! Formally, the phrase is “any real number”, or “lambda is a member of the set of real numbers”, denoted \\(\\lambda \\in \\mathbb{R}\\)↩ Formally, this is a mathematical property called closure↩ "]
]
